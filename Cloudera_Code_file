datasets ===>MNIST ==> churn.txt,GoogleTrendsData.csv,kmeans_data.txt

================>>>>>>>>>>>>>1_python.py-----------------------------


 # Google Stock Analytics
# ======================
#
# This notebook implements a strategy that uses Google Trends data to
# trade the Dow Jones Industrial Average.

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
from pandas_highcharts.display import display_charts
import seaborn
mpl.rcParams['font.family'] = 'Source Sans Pro'
mpl.rcParams['axes.labelsize'] = '16'

# Ensure we're in the correct directory
import os
  
# Import Data
# ===========
#
# Load data from Google Trends.

data = pd.read_csv('data/GoogleTrendsData.csv', index_col='Date', parse_dates=True)
data.head()

# Show DJIA vs. debt related query volume.
display_charts(data, chart_type="stock", title="DJIA vs. Debt Query Volume", secondary_y="debt")
seaborn.lmplot("debt", "djia", data=data, size=7)

# Detect if search volume is increasing or decreasing in
# any given week by forming a moving average and testing if the current value
# crosses the moving average of the past 3 weeks.
#
# Let's first compute the moving average.

data['debt_mavg'] = data.debt.rolling(window=3, center=False).mean()
data.head()

# Since we want to see if the current value is above the moving average of the
# *preceeding* weeks, we have to shift the moving average timeseries forward by one.

data['debt_mavg'] = data.debt_mavg.shift(1)
data.head()

# Generate Orders
# ===============
#
# We use Google Trends to determine how many searches have been
# carried out for a specific search term such as debt in week,
# where Google defines weeks as ending on a Sunday, relative to the total
# number of searches carried out on Google during that time.
#
# We implement the strategy of selling when debt searchess exceed
# the moving average and buying when debt searchers fall below the moving
# average.

data['order'] = 0
data.loc[data.debt > data.debt_mavg, 'order'] = -1
data.loc[data.debt < data.debt_mavg, 'order'] = -1
data.head()

# Compute Returns
# ===============

data['ret_djia'] = data.djia.pct_change()
data.head()

# Returns at week `t` are relative to week `t-1`. However, we are buying at
# week `t` and selling at week `t+1`, so we have to adjust by shifting
# the returns upward.

data['ret_djia'] = data['ret_djia'].shift(-1)

# The algorithm that is used by the authors makes a decision every Monday of
# whether to long or short the Dow Jones. After this week passed, we exit all
# positions (sell if we longed, buy if we shorted) and make a new trading0
# decision.
#
# The $ret$ column contains the weekly returns. Thus, if we buy at week $t$ sell
# at week $t+1$ we make the returns of week $t+1$. Conversely, if we short at
# week $t$ and buy back at week $t+1$ we make the negative returns of week $t+1$."

data['ret_google'] = data.order * data.ret_djia
data['cumulative_google'] = data.ret_google.cumsum()
data['cumulative_djia'] = data.ret_djia.cumsum()

display_charts(data[["cumulative_google", "cumulative_djia"]], 
               title="Cumulative Return for DJIA vs. Google Strategy")


# Script completed!
# ======================
#
# This example code showcases:
# - Markdown via comment
# - Jupyter-compatible visualizations
# - Simple console sharing






==================================================================================================================
================== pyspark.py =====================
# # K-Means
#
# The K-means algorithm written from scratch against PySpark. In practice,
# one may prefer to use the KMeans algorithm in ML, as shown in
# [this example](https://github.com/apache/spark/blob/master/examples/src/main/python/ml/kmeans_example.py).

from __future__ import print_function
import os
import sys
import numpy as np
from pyspark.sql import SparkSession

def parseVector(line):
    return np.array([float(x) for x in line.split(' ')])

def closestPoint(p, centers):
    bestIndex = 0
    closest = float("+inf")
    for i in range(len(centers)):
        tempDist = np.sum((p - centers[i]) ** 2)
        if tempDist < closest:
            closest = tempDist
            bestIndex = i
    return bestIndex

spark = SparkSession\
    .builder\
    .appName("PythonKMeans")\
    .getOrCreate()


  
# Add the data file to hdfs.
!hdfs dfs -put -f $HOME/data/kmeans_data.txt /user/$HADOOP_USER_NAME

lines = spark.read.text("/user/" + os.environ['HADOOP_USER_NAME'] + "/kmeans_data.txt").rdd.map(lambda r: r[0])
data = lines.map(parseVector).cache()
K = 2
convergeDist = 0.1

kPoints = data.takeSample(False, K, 1)
tempDist = 1.0

while tempDist > convergeDist:
    closest = data.map(
        lambda p: (closestPoint(p, kPoints), (p, 1)))
    pointStats = closest.reduceByKey(
        lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))
    newPoints = pointStats.map(
        lambda st: (st[0], st[1][0] / st[1][1])).collect()

    tempDist = sum(np.sum((kPoints[iK] - p) ** 2) for (iK, p) in newPoints)

    for (iK, p) in newPoints:
        kPoints[iK] = p

print("Final centers: " + str(kPoints))

spark.stop()

# Script completed!
# ======================
#
# This example code showcases:
# - Easy connectivity to (kerberized) Spark in YARN client mode.
# - Access to Hadoop HDFS CLI (e.g. `hdfs dfs -ls /`).
=======================================================================================================================
=========================================== Tensorflow.py==================================
import tensorflow as tf
import numpy as np
import matplotlib
from matplotlib import pyplot as plt
import utils

### Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('data/MNIST', one_hot=True)

### View Data
for i in range(0, 3):
  tmp = mnist.train.images[i]
  tmp = tmp.reshape((28,28))
  plt.imshow(tmp, cmap = plt.cm.Greys)
  plt.show()

### Parameters
learning_rate = 0.01
training_epochs = 5
batch_size = 100
display_step = 1
logs_path = '/tmp/tensorboard'

### Cleanup old logs
if tf.gfile.Exists(logs_path):
  tf.gfile.DeleteRecursively(logs_path)
tf.gfile.MakeDirs(logs_path)

### Model
# Use a single-layer perceptron as example $pred = softmax(W x+b)$.
x = tf.placeholder('float', [None, 784], name='data')
y = tf.placeholder('float', [None, 10], name='label')

# Model bias and weight variables: W, b
W = tf.Variable(tf.zeros([784,10]), name='weights')
b = tf.Variable(tf.zeros([10]), name='bias')

# Put the model ops into scopes for tensorboard
with tf.name_scope('Model'):
    logits = tf.matmul(x,W)+b
    pred = tf.nn.softmax( logits )
with tf.name_scope('Loss'):
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))
with tf.name_scope('sgd'):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
with tf.name_scope('evaluation'):
    corr_pred = tf.equal( tf.argmax(pred,1), tf.argmax(y,1) )
    acc = tf.reduce_mean(tf.cast(corr_pred, 'float'))

init = tf.global_variables_initializer()

### Summaries
# Create *summary* ops to monitor the cost/accuracy

loss_summary = tf.summary.scalar('loss', cost)
accu_summary = tf.summary.scalar('accuracy', acc)

merged_summary_op = tf.summary.merge([loss_summary, accu_summary])

## Fit Model
sess = tf.Session()
sess.run(init)

# Write tensboard summaries
summary_writer = tf.summary.FileWriter(logdir=logs_path, graph=tf.get_default_graph())

for epoch in range(1, training_epochs+1):
    avg_cost = 0
    total_batch = int(mnist.train.num_examples/batch_size)
    for i in range(total_batch):
        xs, ys = mnist.train.next_batch(batch_size)
        _, c, summary = sess.run([optimizer, cost, merged_summary_op],
                                feed_dict = {x:xs, y:ys})
        summary_writer.add_summary(summary, epoch*total_batch+i)
        avg_cost += c/total_batch
    if epoch % display_step == 0:
        print('epoch %4d, cost = %.9f' % (epoch, avg_cost))
print("Accuracy: %f" % acc.eval(session=sess, feed_dict={x: mnist.test.images, y: mnist.test.labels}))
summary_writer.close()

### Examine layers    
# A red/black/blue colormap
cdict = {'red':   [(0.0,  1.0, 1.0),
                    (0.25,  1.0, 1.0),
                    (0.5,  0.0, 0.0),
                    (1.0,  0.0, 0.0)],
        'green': [(0.0,  0.0, 0.0),
                    (1.0,  0.0, 0.0)],
        'blue':  [(0.0,  0.0, 0.0),
                       (0.5,  0.0, 0.0),
                       (0.75, 1.0, 1.0),
                       (1.0,  1.0, 1.0)]}
redblue = matplotlib.colors.LinearSegmentedColormap('red_black_blue',cdict,256)

wts = W.eval(sess)
for i in range(0,5):
    im = wts.flatten()[i::10].reshape((28,-1))
    plt.imshow(im, cmap = redblue, clim=(-1.0, 1.0))
    plt.colorbar()
    print("Digit %d" % i)
    plt.show()

### Explore using Tensorboard
utils.start_tensorboard(logs_path, iframe=False)

# Script completed! Click on the above link!
# ======================
#
# This example code showcases:
# - Ability to install and use custom packages (e.g. `pip search tensorflow`)
==========================================================================================================================
======================================== experiment.py ======================================
import sys
import cdsw

args = len(sys.argv) - 1  
sum = 0
x = 1

while (args >= x): 
    print ("Argument %i: %s" % (x, sys.argv[x]))
    sum = sum + int(sys.argv[x])
    x = x + 1
    
print ("Sum of the numbers is: %i." % sum)
cdsw.track_metric("Sum", sum)
======================================================================================================================
========================================== fit.py============================
# Fit a simple linear regression model to the
# classic iris flower dataset to predict petal
# width from petal length. Write the fitted
# model to the file model.pkl

from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
import pickle
import cdsw
import matplotlib.pyplot as plt

iris = datasets.load_iris()
test_size = 20

# Train
iris_x = iris.data[test_size:, 2].reshape(-1, 1) # petal length
iris_y = iris.data[test_size:, 3].reshape(-1, 1) # petal width

model = linear_model.LinearRegression()
model.fit(iris_x, iris_y)

# Test and predict
score_x = iris.data[:test_size, 2].reshape(-1, 1) # petal length
score_y = iris.data[:test_size, 3].reshape(-1, 1) # petal width

predictions = model.predict(score_x)

# Mean squared error
mean_sq = mean_squared_error(score_y, predictions)
cdsw.track_metric("mean_sq_err", mean_sq)
print("Mean squared error: %.2f"% mean_sq)

# Explained variance
r2 = r2_score(score_y, predictions)
cdsw.track_metric("r2", r2)
print('Variance score: %.2f' % r2)

# Output
filename = 'petalWidthModel.pkl'
pickle.dump(model, open(filename, 'wb'))
cdsw.track_file(filename)
=======================================================================================================
================================ predict.py ==================================
# Read the fitted model from the file model.pkl
# and define a function that uses the model to
# predict petal width from petal length

import pickle

model = pickle.load(open('petalWidthModel.pkl', 'rb'))

def predict(args):
  iris_x = float(args.get('petal_length'))
  result = model.predict([[iris_x]])
  return result[0][0]
=====================================================================================================================
============================ dsfortelco_sklearn_exp.py ============================================================
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, average_precision_score
import numpy as np
import pandas as pd
import pickle
import cdsw
import os
import time

spark = SparkSession.builder \
      .appName("Telco Customer Churn") \
      .getOrCreate()
    
schemaData = StructType([StructField("state", StringType(), True),StructField("account_length", DoubleType(), True),StructField("area_code", StringType(), True),StructField("phone_number", StringType(), True),StructField("intl_plan", StringType(), True),StructField("voice_mail_plan", StringType(), True),StructField("number_vmail_messages", DoubleType(), True),     StructField("total_day_minutes", DoubleType(), True),     StructField("total_day_calls", DoubleType(), True),     StructField("total_day_charge", DoubleType(), True),     StructField("total_eve_minutes", DoubleType(), True),     StructField("total_eve_calls", DoubleType(), True),     StructField("total_eve_charge", DoubleType(), True),     StructField("total_night_minutes", DoubleType(), True),     StructField("total_night_calls", DoubleType(), True),     StructField("total_night_charge", DoubleType(), True),     StructField("total_intl_minutes", DoubleType(), True),     StructField("total_intl_calls", DoubleType(), True),     StructField("total_intl_charge", DoubleType(), True),     StructField("number_customer_service_calls", DoubleType(), True),     StructField("churned", StringType(), True)])
churn_data = spark.read.schema(schemaData).csv('/user/' + os.environ['HADOOP_USER_NAME'] + '/churn.txt')

reduced_churn_data= churn_data.select("account_length", "number_vmail_messages", "total_day_calls",
                     "total_day_charge", "total_eve_calls", "total_eve_charge",
                     "total_night_calls", "total_night_charge", "total_intl_calls", 
                    "total_intl_charge","number_customer_service_calls")

label_indexer = StringIndexer(inputCol = 'churned', outputCol = 'label')
plan_indexer = StringIndexer(inputCol = 'intl_plan', outputCol = 'intl_plan_indexed')
pipeline = Pipeline(stages=[plan_indexer, label_indexer])
indexed_data = pipeline.fit(churn_data).transform(churn_data)

(train_data, test_data) = indexed_data.randomSplit([0.7, 0.3])

pdTrain = train_data.toPandas()
pdTest = test_data.toPandas()
features = ["intl_plan_indexed","account_length", "number_vmail_messages", "total_day_calls",
                     "total_day_charge", "total_eve_calls", "total_eve_charge",
                     "total_night_calls", "total_night_charge", "total_intl_calls", 
                    "total_intl_charge","number_customer_service_calls"]

param_numTrees = int(sys.argv[1])
param_maxDepth = int(sys.argv[2])
param_impurity = 'gini'

randF=RandomForestClassifier(n_jobs=10,
                             n_estimators=param_numTrees, 
                             max_depth=param_maxDepth, 
                             criterion = param_impurity,
                             random_state=0)

cdsw.track_metric("numTrees",param_numTrees)
cdsw.track_metric("maxDepth",param_maxDepth)
cdsw.track_metric("impurity",param_impurity)

randF.fit(pdTrain[features], pdTrain['label'])

predictions=randF.predict(pdTest[features])

#temp = randF.predict_proba(pdTest[features])

pd.crosstab(pdTest['label'], predictions, rownames=['Actual'], colnames=['Prediction'])

list(zip(pdTrain[features], randF.feature_importances_))


y_true = pdTest['label']
y_scores = predictions
auroc = roc_auc_score(y_true, y_scores)
ap = average_precision_score (y_true, y_scores)
print(auroc, ap)

cdsw.track_metric("auroc", auroc)
cdsw.track_metric("ap", ap)

pickle.dump(randF, open("sklearn_rf.pkl","wb"))

cdsw.track_file("sklearn_rf.pkl")

time.sleep(20)
print("Slept for 20 seconds.")
========================================================================================================================
=====================================predict_churn_sklearn.py=======================================================
import pickle
import numpy as np

model = pickle.load(open('sklearn_rf.pkl', 'rb'))

def predict(args):
  account=np.array(args["feature"].split(",")).reshape(1,-1)
  return {"result" : model.predict(account)[0]}
  
==================================================================================================================
======================================== readme.md ==============================================
# Cloudera Data Science Workbench

Basic tour of Cloudera Data Science Workbench (CDSW).

## Initial setup

Open the Workbench and start a Python3 Session, then run the following command to install some required libraries:
```
!pip3 install --upgrade pip dask keras matplotlib==2.0.0. pandas_highcharts protobuf tensorflow==1.3.0. seaborn sklearn numpy
```

Please note: as libraries are updated, dependancies might break: please check the output log and fix the dependancies. For example, you might have to install a specific version of setuptools or numpy for tensorflow to install properly.

Start a new R Session, and run these commands:
```
install.packages('sparklyr')
install.packages('plotly')
install.packages("nycflights13")
install.packages("Lahman")
install.packages("mgcv")
install.packages('shiny') 
```

Stop and restart all sessions for changes to take effect.

Want more Experiment and Models examples? Check our [secret excercise](Experiment-and-Models.md) 
=====================================================================================================================
==================================== utils.py ========================================================================
import os
import time
from IPython.core.display import HTML, display
import subprocess
process = False

def start_tensorboard(logdir="/tmp/tensorboard", iframe=True):
  "Starts tensorboard on public web port for session."  
  cmd = ["python3", "/home/cdsw/.local/bin/tensorboard",
    "--logdir=" + logdir, "--port=8080"]
  global process
  if not process:
    process = subprocess.Popen(cmd)
    time.sleep(3)    
  url = "http://{id}.{domain}".format(id=os.environ['CDSW_ENGINE_ID'], domain=os.environ['CDSW_DOMAIN'])
  print ("Starting Tensorboard at {url}...".format(url=url))
  if iframe:
    html = """
      <p><a href="{url}">Open Tensorboard</a></p>
      <iframe  width="100%" height=700px" style="border: 0" src="{url}" seamless></iframe>
    """.format(url=url)
  else:
    html = """
      <p><a href="{url}">Open Tensorboard</a></p>
    """.format(url=url)
  display(HTML(html))

def stop_tensorboard():
  "Stop tensorboard"
  global process
  if process: 
    process.terminate()
    print ("Tensorboard stopped.")
    process = False
  else:
    print ("Tensorboard is not running.")
=========================================================================================================================
